=========== start train the age prediction model ===========

 ==========> Using 8 processes for data loader.
(3712,) 3712
(752,) 752
 ==========> Training is getting started...
 ==========> Training takes 150 epochs.
Epoch: [0 / 150]   [step 0/232]	Loss1 1337.863 (1337.863)	Loss2 215.132 (215.132)	Loss 3489.1821 (3489.1821)	MAE 31.676 (31.676)	
Epoch: [0 / 150]   [step 40/232]	Loss1 219.061 (653.920)	Loss2 422.560 (418.058)	Loss 4444.6611 (4834.4946)	MAE 11.674 (20.954)	
Epoch: [0 / 150]   [step 80/232]	Loss1 237.774 (440.684)	Loss2 353.954 (377.954)	Loss 3777.3096 (4220.2217)	MAE 11.691 (16.674)	
Epoch: [0 / 150]   [step 120/232]	Loss1 164.102 (349.235)	Loss2 156.859 (343.022)	Loss 1732.6915 (3779.4504)	MAE 10.854 (14.680)	
Epoch: [0 / 150]   [step 160/232]	Loss1 169.432 (302.646)	Loss2 281.861 (324.924)	Loss 2988.0400 (3551.8848)	MAE 10.542 (13.620)	
Epoch: [0 / 150]   [step 200/232]	Loss1 78.725 (278.878)	Loss2 198.524 (317.792)	Loss 2063.9661 (3456.7966)	MAE 6.947 (13.083)	
Valid: [steps 47], Loss 2116.2717,  MAE:  10.1813

*learning rate 1.00e-05*

=======>   Best at epoch 0, valid MAE 10.181295

=======>   This is the best model !!! It has been saved!!!!!!


Epoch: [1 / 150]   [step 0/232]	Loss1 147.366 (147.366)	Loss2 150.154 (150.154)	Loss 1648.9102 (1648.9102)	MAE 10.457 (10.457)	
Epoch: [1 / 150]   [step 40/232]	Loss1 136.501 (173.213)	Loss2 212.406 (252.304)	Loss 2260.5654 (2696.2559)	MAE 8.955 (10.548)	
Epoch: [1 / 150]   [step 80/232]	Loss1 370.332 (169.941)	Loss2 488.552 (261.689)	Loss 5255.8535 (2786.8333)	MAE 16.073 (10.263)	
Epoch: [1 / 150]   [step 120/232]	Loss1 81.618 (170.929)	Loss2 122.529 (259.935)	Loss 1306.9052 (2770.2827)	MAE 6.867 (10.377)	
Epoch: [1 / 150]   [step 160/232]	Loss1 142.217 (166.426)	Loss2 397.257 (250.828)	Loss 4114.7842 (2674.7029)	MAE 9.527 (10.260)	
Epoch: [1 / 150]   [step 200/232]	Loss1 241.133 (167.373)	Loss2 373.073 (254.754)	Loss 3971.8652 (2714.9150)	MAE 13.287 (10.305)	
Valid: [steps 47], Loss 2563.6125,  MAE:  19.3184

*learning rate 1.00e-05*

EarlyStopping counter: 1 out of 20


Epoch: [2 / 150]   [step 0/232]	Loss1 198.079 (198.079)	Loss2 265.712 (265.712)	Loss 2855.1978 (2855.1978)	MAE 11.848 (11.848)	
Epoch: [2 / 150]   [step 40/232]	Loss1 142.329 (144.971)	Loss2 188.213 (199.220)	Loss 2024.4580 (2137.1733)	MAE 9.709 (9.728)	
Epoch: [2 / 150]   [step 80/232]	Loss1 111.721 (149.747)	Loss2 215.107 (212.884)	Loss 2262.7925 (2278.5842)	MAE 8.802 (9.802)	
Epoch: [2 / 150]   [step 120/232]	Loss1 96.717 (145.566)	Loss2 133.203 (211.412)	Loss 1428.7523 (2259.6821)	MAE 8.579 (9.712)	
Epoch: [2 / 150]   [step 160/232]	Loss1 155.367 (147.223)	Loss2 204.238 (213.085)	Loss 2197.7463 (2278.0771)	MAE 9.669 (9.696)	
Epoch: [2 / 150]   [step 200/232]	Loss1 286.171 (145.432)	Loss2 133.748 (211.929)	Loss 1623.6525 (2264.7180)	MAE 15.523 (9.660)	
Valid: [steps 47], Loss 2015.7034,  MAE:  10.0800

*learning rate 1.00e-05*

=======>   Best at epoch 2, valid MAE 10.079985

=======>   This is the best model !!! It has been saved!!!!!!


Epoch: [3 / 150]   [step 0/232]	Loss1 214.053 (214.053)	Loss2 260.142 (260.142)	Loss 2815.4692 (2815.4692)	MAE 12.257 (12.257)	
Epoch: [3 / 150]   [step 40/232]	Loss1 85.959 (134.909)	Loss2 201.798 (190.479)	Loss 2103.9368 (2039.6991)	MAE 6.694 (9.203)	
Epoch: [3 / 150]   [step 80/232]	Loss1 94.880 (125.501)	Loss2 111.420 (192.433)	Loss 1209.0770 (2049.8333)	MAE 7.420 (8.906)	
Epoch: [3 / 150]   [step 120/232]	Loss1 129.112 (128.015)	Loss2 259.792 (192.379)	Loss 2727.0342 (2051.8015)	MAE 9.560 (9.022)	
Epoch: [3 / 150]   [step 160/232]	Loss1 130.185 (129.569)	Loss2 255.261 (188.879)	Loss 2682.7981 (2018.3601)	MAE 9.388 (9.084)	
Epoch: [3 / 150]   [step 200/232]	Loss1 182.560 (128.635)	Loss2 281.885 (186.251)	Loss 3001.4136 (1991.1455)	MAE 10.587 (9.047)	
Valid: [steps 47], Loss 1564.4833,  MAE:  9.3341

*learning rate 1.00e-05*

=======>   Best at epoch 3, valid MAE 9.334096

=======>   This is the best model !!! It has been saved!!!!!!


Epoch: [4 / 150]   [step 0/232]	Loss1 73.697 (73.697)	Loss2 87.353 (87.353)	Loss 947.2224 (947.2224)	MAE 7.310 (7.310)	
Epoch: [4 / 150]   [step 40/232]	Loss1 83.515 (101.498)	Loss2 155.477 (153.634)	Loss 1638.2882 (1637.8378)	MAE 7.603 (8.097)	
Epoch: [4 / 150]   [step 80/232]	Loss1 119.464 (107.914)	Loss2 101.980 (166.847)	Loss 1139.2654 (1776.3812)	MAE 8.872 (8.381)	
Epoch: [4 / 150]   [step 120/232]	Loss1 47.142 (105.092)	Loss2 54.557 (157.472)	Loss 592.7077 (1679.8146)	MAE 4.265 (8.190)	
Epoch: [4 / 150]   [step 160/232]	Loss1 47.483 (107.274)	Loss2 37.464 (156.307)	Loss 422.1221 (1670.3463)	MAE 6.046 (8.209)	
Epoch: [4 / 150]   [step 200/232]	Loss1 88.279 (103.202)	Loss2 124.563 (155.305)	Loss 1333.9124 (1656.2478)	MAE 7.684 (8.066)	
Valid: [steps 47], Loss 1449.1328,  MAE:  12.3321

*learning rate 1.00e-05*

EarlyStopping counter: 1 out of 20


Epoch: [5 / 150]   [step 0/232]	Loss1 135.908 (135.908)	Loss2 270.595 (270.595)	Loss 2841.8579 (2841.8579)	MAE 9.001 (9.001)	
Epoch: [5 / 150]   [step 40/232]	Loss1 86.323 (100.955)	Loss2 153.374 (134.329)	Loss 1620.0626 (1444.2487)	MAE 7.527 (7.805)	
Epoch: [5 / 150]   [step 80/232]	Loss1 162.615 (97.829)	Loss2 92.324 (146.450)	Loss 1085.8586 (1562.3306)	MAE 11.142 (7.753)	
Epoch: [5 / 150]   [step 120/232]	Loss1 52.352 (92.709)	Loss2 106.908 (147.023)	Loss 1121.4359 (1562.9381)	MAE 5.935 (7.593)	
Epoch: [5 / 150]   [step 160/232]	Loss1 79.729 (93.132)	Loss2 98.608 (141.085)	Loss 1065.8107 (1503.9854)	MAE 6.393 (7.648)	
Epoch: [5 / 150]   [step 200/232]	Loss1 83.799 (93.459)	Loss2 97.456 (143.849)	Loss 1058.3574 (1531.9486)	MAE 7.241 (7.678)	
Valid: [steps 47], Loss 1324.4717,  MAE:  7.3757

*learning rate 1.00e-05*

=======>   Best at epoch 5, valid MAE 7.375657

=======>   This is the best model !!! It has been saved!!!!!!


Epoch: [6 / 150]   [step 0/232]	Loss1 34.309 (34.309)	Loss2 58.232 (58.232)	Loss 616.6315 (616.6315)	MAE 5.004 (5.004)	
Epoch: [6 / 150]   [step 40/232]	Loss1 84.321 (86.475)	Loss2 13.106 (116.945)	Loss 215.3789 (1255.9293)	MAE 7.224 (7.240)	
Epoch: [6 / 150]   [step 80/232]	Loss1 61.230 (81.818)	Loss2 127.095 (114.852)	Loss 1332.1781 (1230.3368)	MAE 6.362 (7.129)	
Epoch: [6 / 150]   [step 120/232]	Loss1 58.209 (83.974)	Loss2 135.399 (118.560)	Loss 1412.2035 (1269.5708)	MAE 6.170 (7.230)	
Epoch: [6 / 150]   [step 160/232]	Loss1 296.315 (84.769)	Loss2 120.576 (120.851)	Loss 1502.0743 (1293.2750)	MAE 15.043 (7.277)	
Epoch: [6 / 150]   [step 200/232]	Loss1 104.624 (85.208)	Loss2 164.789 (122.182)	Loss 1752.5121 (1307.0286)	MAE 8.139 (7.275)	
Valid: [steps 47], Loss 1319.0458,  MAE:  7.6895

*learning rate 1.00e-05*

EarlyStopping counter: 1 out of 20


Epoch: [7 / 150]   [step 0/232]	Loss1 62.531 (62.531)	Loss2 120.306 (120.306)	Loss 1265.5896 (1265.5896)	MAE 7.016 (7.016)	
Epoch: [7 / 150]   [step 40/232]	Loss1 41.918 (75.064)	Loss2 74.019 (106.281)	Loss 782.1068 (1137.8724)	MAE 5.733 (7.143)	
Epoch: [7 / 150]   [step 80/232]	Loss1 59.458 (77.485)	Loss2 54.698 (110.426)	Loss 606.4364 (1181.7482)	MAE 6.327 (7.159)	
Epoch: [7 / 150]   [step 120/232]	Loss1 88.859 (79.817)	Loss2 90.638 (106.830)	Loss 995.2348 (1148.1200)	MAE 8.505 (7.192)	
Epoch: [7 / 150]   [step 160/232]	Loss1 94.334 (84.084)	Loss2 57.187 (107.485)	Loss 666.2034 (1158.9329)	MAE 8.231 (7.285)	
Epoch: [7 / 150]   [step 200/232]	Loss1 87.499 (80.515)	Loss2 52.739 (105.071)	Loss 614.8910 (1131.2212)	MAE 8.144 (7.112)	
Valid: [steps 47], Loss 1076.8851,  MAE:  7.3451

*learning rate 1.00e-05*

=======>   Best at epoch 7, valid MAE 7.345089

=======>   This is the best model !!! It has been saved!!!!!!


Epoch: [8 / 150]   [step 0/232]	Loss1 57.741 (57.741)	Loss2 71.840 (71.840)	Loss 776.1431 (776.1431)	MAE 6.151 (6.151)	
Epoch: [8 / 150]   [step 40/232]	Loss1 50.649 (61.732)	Loss2 86.437 (92.910)	Loss 915.0181 (990.8274)	MAE 5.723 (6.291)	
Epoch: [8 / 150]   [step 80/232]	Loss1 31.123 (64.232)	Loss2 105.313 (91.135)	Loss 1084.2573 (975.5796)	MAE 4.398 (6.284)	
Epoch: [8 / 150]   [step 120/232]	Loss1 54.437 (63.986)	Loss2 110.570 (84.774)	Loss 1160.1350 (911.7249)	MAE 5.298 (6.220)	
Epoch: [8 / 150]   [step 160/232]	Loss1 70.556 (63.353)	Loss2 106.906 (85.924)	Loss 1139.6157 (922.5917)	MAE 6.090 (6.206)	
Epoch: [8 / 150]   [step 200/232]	Loss1 91.824 (64.375)	Loss2 160.624 (88.504)	Loss 1698.0599 (949.4163)	MAE 7.878 (6.252)	
Valid: [steps 47], Loss 1133.6685,  MAE:  6.9714

*learning rate 1.00e-05*

=======>   Best at epoch 8, valid MAE 6.971414

=======>   This is the best model !!! It has been saved!!!!!!


Epoch: [9 / 150]   [step 0/232]	Loss1 54.115 (54.115)	Loss2 129.503 (129.503)	Loss 1349.1455 (1349.1455)	MAE 6.585 (6.585)	
Epoch: [9 / 150]   [step 40/232]	Loss1 42.433 (51.067)	Loss2 80.122 (81.654)	Loss 843.6567 (867.6028)	MAE 5.019 (5.626)	
Epoch: [9 / 150]   [step 80/232]	Loss1 260.687 (55.146)	Loss2 56.067 (85.583)	Loss 821.3577 (910.9802)	MAE 14.995 (5.831)	
Epoch: [9 / 150]   [step 120/232]	Loss1 68.259 (58.442)	Loss2 78.441 (84.011)	Loss 852.6734 (898.5501)	MAE 6.598 (6.025)	
Epoch: [9 / 150]   [step 160/232]	Loss1 16.842 (58.123)	Loss2 13.910 (79.628)	Loss 155.9396 (854.4056)	MAE 3.661 (6.023)	
Epoch: [9 / 150]   [step 200/232]	Loss1 68.856 (57.879)	Loss2 93.238 (79.278)	Loss 1001.2393 (850.6625)	MAE 6.456 (6.024)	
Valid: [steps 47], Loss 1354.7516,  MAE:  14.2389

*learning rate 1.00e-05*

EarlyStopping counter: 1 out of 20


Epoch: [10 / 150]   [step 0/232]	Loss1 51.967 (51.967)	Loss2 66.765 (66.765)	Loss 719.6131 (719.6131)	MAE 6.233 (6.233)	
Epoch: [10 / 150]   [step 40/232]	Loss1 108.327 (76.165)	Loss2 102.035 (87.326)	Loss 1128.6812 (949.4231)	MAE 9.191 (6.823)	
Epoch: [10 / 150]   [step 80/232]	Loss1 67.617 (65.216)	Loss2 177.252 (74.550)	Loss 1840.1365 (810.7203)	MAE 6.507 (6.339)	
Epoch: [10 / 150]   [step 120/232]	Loss1 54.404 (61.033)	Loss2 85.988 (74.005)	Loss 914.2808 (801.0781)	MAE 5.456 (6.162)	
Epoch: [10 / 150]   [step 160/232]	Loss1 28.018 (60.279)	Loss2 29.342 (71.847)	Loss 321.4408 (778.7518)	MAE 4.321 (6.127)	
Epoch: [10 / 150]   [step 200/232]	Loss1 56.816 (59.160)	Loss2 78.938 (71.258)	Loss 846.1935 (771.7371)	MAE 5.966 (6.093)	
Valid: [steps 47], Loss 1104.2896,  MAE:  9.9153

*learning rate 1.00e-05*

EarlyStopping counter: 2 out of 20


Epoch: [11 / 150]   [step 0/232]	Loss1 67.067 (67.067)	Loss2 24.425 (24.425)	Loss 311.3193 (311.3193)	MAE 6.992 (6.992)	
Epoch: [11 / 150]   [step 40/232]	Loss1 18.573 (46.090)	Loss2 15.010 (55.480)	Loss 168.6780 (600.8880)	MAE 3.673 (5.342)	
Epoch: [11 / 150]   [step 80/232]	Loss1 83.619 (42.177)	Loss2 130.279 (54.827)	Loss 1386.4045 (590.4501)	MAE 7.065 (5.080)	
Epoch: [11 / 150]   [step 120/232]	Loss1 35.812 (45.447)	Loss2 99.001 (55.653)	Loss 1025.8241 (601.9768)	MAE 4.734 (5.308)	
Epoch: [11 / 150]   [step 160/232]	Loss1 118.307 (49.190)	Loss2 183.203 (60.814)	Loss 1950.3368 (657.3307)	MAE 9.212 (5.526)	
Epoch: [11 / 150]   [step 200/232]	Loss1 51.911 (49.164)	Loss2 99.854 (58.818)	Loss 1050.4509 (637.3433)	MAE 5.791 (5.537)	
Valid: [steps 47], Loss 1179.6930,  MAE:  13.9722

*learning rate 1.00e-05*

EarlyStopping counter: 3 out of 20


Epoch: [12 / 150]   [step 0/232]	Loss1 91.223 (91.223)	Loss2 60.024 (60.024)	Loss 691.4642 (691.4642)	MAE 8.313 (8.313)	
Epoch: [12 / 150]   [step 40/232]	Loss1 31.930 (43.128)	Loss2 44.205 (53.451)	Loss 473.9802 (577.6405)	MAE 4.899 (5.273)	
Epoch: [12 / 150]   [step 80/232]	Loss1 39.082 (47.064)	Loss2 39.265 (58.442)	Loss 431.7273 (631.4833)	MAE 4.895 (5.516)	
Epoch: [12 / 150]   [step 120/232]	Loss1 36.151 (43.549)	Loss2 91.757 (53.662)	Loss 953.7210 (580.1729)	MAE 4.855 (5.283)	
Epoch: [12 / 150]   [step 160/232]	Loss1 18.134 (44.124)	Loss2 10.064 (53.305)	Loss 118.7788 (577.1793)	MAE 3.512 (5.326)	
Epoch: [12 / 150]   [step 200/232]	Loss1 50.941 (45.845)	Loss2 83.768 (53.819)	Loss 888.6196 (584.0397)	MAE 5.288 (5.402)	
Valid: [steps 47], Loss 1197.3123,  MAE:  16.0582

*learning rate 1.00e-05*

EarlyStopping counter: 4 out of 20


Epoch: [13 / 150]   [step 0/232]	Loss1 30.865 (30.865)	Loss2 68.196 (68.196)	Loss 712.8237 (712.8237)	MAE 4.229 (4.229)	
Epoch: [13 / 150]   [step 40/232]	Loss1 52.519 (33.583)	Loss2 69.982 (41.150)	Loss 752.3401 (445.0794)	MAE 6.210 (4.668)	
Epoch: [13 / 150]   [step 80/232]	Loss1 54.792 (40.576)	Loss2 57.849 (46.430)	Loss 633.2806 (504.8802)	MAE 6.479 (5.068)	
Epoch: [13 / 150]   [step 120/232]	Loss1 43.486 (40.686)	Loss2 49.961 (43.411)	Loss 543.0975 (474.7996)	MAE 4.940 (5.058)	
Epoch: [13 / 150]   [step 160/232]	Loss1 24.921 (40.417)	Loss2 54.791 (43.827)	Loss 572.8286 (478.6892)	MAE 4.355 (5.057)	
Epoch: [13 / 150]   [step 200/232]	Loss1 17.415 (41.147)	Loss2 13.549 (42.991)	Loss 152.9036 (471.0596)	MAE 3.231 (5.110)	
Valid: [steps 47], Loss 1443.0745,  MAE:  22.7838

*learning rate 1.00e-05*

EarlyStopping counter: 5 out of 20


Epoch: [14 / 150]   [step 0/232]	Loss1 17.109 (17.109)	Loss2 27.582 (27.582)	Loss 292.9263 (292.9263)	MAE 3.562 (3.562)	
Epoch: [14 / 150]   [step 40/232]	Loss1 23.678 (25.560)	Loss2 39.878 (33.431)	Loss 422.4548 (359.8739)	MAE 4.002 (4.075)	
Epoch: [14 / 150]   [step 80/232]	Loss1 45.332 (27.744)	Loss2 89.534 (31.577)	Loss 940.6678 (343.5131)	MAE 5.389 (4.211)	
Epoch: [14 / 150]   [step 120/232]	Loss1 28.039 (28.575)	Loss2 22.364 (32.926)	Loss 251.6793 (357.8319)	MAE 4.797 (4.256)	
Epoch: [14 / 150]   [step 160/232]	Loss1 11.311 (32.973)	Loss2 20.737 (37.334)	Loss 218.6795 (406.3174)	MAE 2.948 (4.525)	
Epoch: [14 / 150]   [step 200/232]	Loss1 73.772 (33.921)	Loss2 26.528 (37.341)	Loss 339.0542 (407.3350)	MAE 7.615 (4.604)	
Valid: [steps 47], Loss 986.5829,  MAE:  8.7136

*learning rate 5.00e-06*

EarlyStopping counter: 6 out of 20


Epoch: [15 / 150]   [step 0/232]	Loss1 13.071 (13.071)	Loss2 10.526 (10.526)	Loss 118.3345 (118.3345)	MAE 2.669 (2.669)	
Epoch: [15 / 150]   [step 40/232]	Loss1 18.041 (22.149)	Loss2 13.723 (19.540)	Loss 155.2662 (217.5469)	MAE 3.507 (3.674)	
Epoch: [15 / 150]   [step 80/232]	Loss1 16.517 (20.965)	Loss2 15.507 (19.403)	Loss 171.5919 (214.9995)	MAE 3.326 (3.602)	
Epoch: [15 / 150]   [step 120/232]	Loss1 28.063 (20.298)	Loss2 8.299 (19.082)	Loss 111.0556 (211.1155)	MAE 4.539 (3.557)	
Epoch: [15 / 150]   [step 160/232]	Loss1 9.573 (20.569)	Loss2 11.035 (19.054)	Loss 119.9231 (211.1114)	MAE 2.586 (3.576)	
Epoch: [15 / 150]   [step 200/232]	Loss1 7.871 (20.385)	Loss2 13.981 (18.593)	Loss 147.6790 (206.3103)	MAE 2.235 (3.566)	
Valid: [steps 47], Loss 907.2425,  MAE:  7.2293

*learning rate 5.00e-06*

EarlyStopping counter: 7 out of 20


Epoch: [16 / 150]   [step 0/232]	Loss1 9.420 (9.420)	Loss2 12.126 (12.126)	Loss 130.6786 (130.6786)	MAE 2.285 (2.285)	
Epoch: [16 / 150]   [step 40/232]	Loss1 6.510 (20.895)	Loss2 3.665 (12.399)	Loss 43.1572 (144.8876)	MAE 2.125 (3.522)	
Epoch: [16 / 150]   [step 80/232]	Loss1 4.820 (18.444)	Loss2 10.349 (11.843)	Loss 108.3053 (136.8756)	MAE 1.815 (3.342)	
Epoch: [16 / 150]   [step 120/232]	Loss1 24.539 (16.706)	Loss2 4.760 (11.554)	Loss 72.1402 (132.2469)	MAE 4.573 (3.190)	
Epoch: [16 / 150]   [step 160/232]	Loss1 11.007 (16.944)	Loss2 20.317 (12.210)	Loss 214.1815 (139.0471)	MAE 2.228 (3.237)	
Epoch: [16 / 150]   [step 200/232]	Loss1 7.586 (16.077)	Loss2 8.133 (12.319)	Loss 88.9172 (139.2631)	MAE 2.115 (3.169)	
Valid: [steps 47], Loss 869.6835,  MAE:  5.3039

*learning rate 5.00e-06*

=======>   Best at epoch 16, valid MAE 5.303870

=======>   This is the best model !!! It has been saved!!!!!!


Epoch: [17 / 150]   [step 0/232]	Loss1 8.909 (8.909)	Loss2 15.272 (15.272)	Loss 161.6308 (161.6308)	MAE 2.234 (2.234)	
Epoch: [17 / 150]   [step 40/232]	Loss1 4.950 (14.012)	Loss2 11.085 (9.634)	Loss 115.7975 (110.3517)	MAE 1.763 (2.995)	
Epoch: [17 / 150]   [step 80/232]	Loss1 6.324 (13.819)	Loss2 16.975 (9.189)	Loss 176.0778 (105.7072)	MAE 1.980 (2.947)	
Epoch: [17 / 150]   [step 120/232]	Loss1 11.411 (14.796)	Loss2 12.482 (9.756)	Loss 136.2291 (112.3519)	MAE 2.767 (3.021)	
Epoch: [17 / 150]   [step 160/232]	Loss1 2.729 (14.283)	Loss2 7.347 (9.515)	Loss 76.1976 (109.4378)	MAE 1.183 (2.951)	
Epoch: [17 / 150]   [step 200/232]	Loss1 23.028 (13.768)	Loss2 4.494 (9.659)	Loss 67.9711 (110.3629)	MAE 4.506 (2.913)	
Valid: [steps 47], Loss 925.4913,  MAE:  6.2543

*learning rate 5.00e-06*

EarlyStopping counter: 1 out of 20


Epoch: [18 / 150]   [step 0/232]	Loss1 17.078 (17.078)	Loss2 3.680 (3.680)	Loss 53.8818 (53.8818)	MAE 3.817 (3.817)	
Epoch: [18 / 150]   [step 40/232]	Loss1 34.071 (16.116)	Loss2 11.225 (8.891)	Loss 146.3172 (105.0300)	MAE 5.434 (3.254)	
Epoch: [18 / 150]   [step 80/232]	Loss1 3.290 (14.016)	Loss2 4.454 (7.715)	Loss 47.8269 (91.1682)	MAE 1.329 (2.986)	
Epoch: [18 / 150]   [step 120/232]	Loss1 27.841 (13.564)	Loss2 8.658 (7.657)	Loss 114.4223 (90.1296)	MAE 4.991 (2.910)	
Epoch: [18 / 150]   [step 160/232]	Loss1 29.572 (13.389)	Loss2 13.009 (7.918)	Loss 159.6583 (92.5673)	MAE 5.028 (2.871)	
Epoch: [18 / 150]   [step 200/232]	Loss1 8.129 (13.148)	Loss2 5.932 (8.046)	Loss 67.4475 (93.6068)	MAE 2.233 (2.858)	
Valid: [steps 47], Loss 833.8556,  MAE:  8.5528

*learning rate 5.00e-06*

EarlyStopping counter: 2 out of 20


Epoch: [19 / 150]   [step 0/232]	Loss1 76.398 (76.398)	Loss2 9.710 (9.710)	Loss 173.5020 (173.5020)	MAE 8.538 (8.538)	
Epoch: [19 / 150]   [step 40/232]	Loss1 5.494 (15.912)	Loss2 7.508 (5.636)	Loss 80.5744 (72.2725)	MAE 2.219 (3.087)	
Epoch: [19 / 150]   [step 80/232]	Loss1 24.518 (13.945)	Loss2 5.412 (5.473)	Loss 78.6408 (68.6698)	MAE 4.554 (2.930)	
Epoch: [19 / 150]   [step 120/232]	Loss1 15.889 (12.225)	Loss2 3.953 (5.591)	Loss 55.4207 (68.1309)	MAE 3.681 (2.735)	
Epoch: [19 / 150]   [step 160/232]	Loss1 2.974 (11.776)	Loss2 4.258 (5.485)	Loss 45.5534 (66.6293)	MAE 1.298 (2.710)	
Epoch: [19 / 150]   [step 200/232]	Loss1 2.441 (11.603)	Loss2 6.810 (5.354)	Loss 70.5407 (65.1432)	MAE 1.271 (2.679)	
Valid: [steps 47], Loss 874.5331,  MAE:  6.1691

*learning rate 5.00e-06*

EarlyStopping counter: 3 out of 20


Epoch: [20 / 150]   [step 0/232]	Loss1 2.199 (2.199)	Loss2 5.785 (5.785)	Loss 60.0528 (60.0528)	MAE 1.221 (1.221)	
Epoch: [20 / 150]   [step 40/232]	Loss1 3.211 (15.335)	Loss2 5.291 (4.960)	Loss 56.1230 (64.9329)	MAE 1.559 (3.064)	
Epoch: [20 / 150]   [step 80/232]	Loss1 5.100 (12.454)	Loss2 3.341 (4.607)	Loss 38.5053 (58.5284)	MAE 1.948 (2.783)	
Epoch: [20 / 150]   [step 120/232]	Loss1 2.762 (12.894)	Loss2 4.393 (4.283)	Loss 46.6918 (55.7187)	MAE 1.401 (2.788)	
Epoch: [20 / 150]   [step 160/232]	Loss1 3.140 (12.359)	Loss2 3.758 (4.216)	Loss 40.7239 (54.5222)	MAE 1.465 (2.769)	
Epoch: [20 / 150]   [step 200/232]	Loss1 5.922 (11.981)	Loss2 3.377 (4.174)	Loss 39.6894 (53.7161)	MAE 2.085 (2.720)	
Valid: [steps 47], Loss 828.6623,  MAE:  5.5917

*learning rate 5.00e-06*

EarlyStopping counter: 4 out of 20


Epoch: [21 / 150]   [step 0/232]	Loss1 2.554 (2.554)	Loss2 3.853 (3.853)	Loss 41.0810 (41.0810)	MAE 1.265 (1.265)	
Epoch: [21 / 150]   [step 40/232]	Loss1 39.390 (8.218)	Loss2 6.329 (2.820)	Loss 102.6789 (36.4191)	MAE 6.141 (2.299)	
Epoch: [21 / 150]   [step 80/232]	Loss1 9.178 (11.805)	Loss2 2.326 (3.068)	Loss 32.4403 (42.4824)	MAE 2.778 (2.791)	
Epoch: [21 / 150]   [step 120/232]	Loss1 2.914 (10.366)	Loss2 1.688 (3.041)	Loss 19.7938 (40.7754)	MAE 1.481 (2.612)	
Epoch: [21 / 150]   [step 160/232]	Loss1 7.106 (9.958)	Loss2 1.570 (3.021)	Loss 22.8009 (40.1685)	MAE 2.407 (2.548)	
Epoch: [21 / 150]   [step 200/232]	Loss1 13.419 (9.976)	Loss2 3.050 (3.073)	Loss 43.9238 (40.7014)	MAE 3.470 (2.527)	
Valid: [steps 47], Loss 746.1863,  MAE:  5.1262

*learning rate 5.00e-06*

=======>   Best at epoch 21, valid MAE 5.126221

=======>   This is the best model !!! It has been saved!!!!!!


Epoch: [22 / 150]   [step 0/232]	Loss1 37.684 (37.684)	Loss2 5.269 (5.269)	Loss 90.3706 (90.3706)	MAE 6.007 (6.007)	
Epoch: [22 / 150]   [step 40/232]	Loss1 34.526 (11.540)	Loss2 0.392 (2.250)	Loss 38.4507 (34.0404)	MAE 5.850 (2.643)	
Epoch: [22 / 150]   [step 80/232]	Loss1 1.703 (11.297)	Loss2 3.189 (2.183)	Loss 33.5961 (33.1311)	MAE 1.149 (2.665)	
Epoch: [22 / 150]   [step 120/232]	Loss1 2.596 (10.457)	Loss2 3.701 (2.178)	Loss 39.6063 (32.2340)	MAE 1.286 (2.560)	
Epoch: [22 / 150]   [step 160/232]	Loss1 27.007 (9.783)	Loss2 1.715 (2.202)	Loss 44.1593 (31.8063)	MAE 5.069 (2.480)	
Epoch: [22 / 150]   [step 200/232]	Loss1 9.792 (9.210)	Loss2 1.940 (2.212)	Loss 29.1895 (31.3313)	MAE 2.968 (2.402)	
Valid: [steps 47], Loss 802.8590,  MAE:  7.2317

*learning rate 5.00e-06*

EarlyStopping counter: 1 out of 20


Epoch: [23 / 150]   [step 0/232]	Loss1 56.291 (56.291)	Loss2 1.646 (1.646)	Loss 72.7470 (72.7470)	MAE 7.461 (7.461)	
Epoch: [23 / 150]   [step 40/232]	Loss1 1.301 (10.250)	Loss2 1.539 (1.809)	Loss 16.6936 (28.3402)	MAE 1.010 (2.562)	
Epoch: [23 / 150]   [step 80/232]	Loss1 6.186 (8.068)	Loss2 1.705 (1.661)	Loss 23.2321 (24.6797)	MAE 2.307 (2.231)	
Epoch: [23 / 150]   [step 120/232]	Loss1 40.802 (8.428)	Loss2 2.621 (1.634)	Loss 67.0152 (24.7686)	MAE 6.331 (2.287)	
Epoch: [23 / 150]   [step 160/232]	Loss1 0.653 (9.076)	Loss2 1.301 (1.657)	Loss 13.6623 (25.6503)	MAE 0.615 (2.364)	
Epoch: [23 / 150]   [step 200/232]	Loss1 10.858 (9.165)	Loss2 2.834 (1.731)	Loss 39.1945 (26.4715)	MAE 3.109 (2.390)	
Valid: [steps 47], Loss 732.4298,  MAE:  5.6860

*learning rate 5.00e-06*

EarlyStopping counter: 2 out of 20


Epoch: [24 / 150]   [step 0/232]	Loss1 18.113 (18.113)	Loss2 1.055 (1.055)	Loss 28.6630 (28.6630)	MAE 4.194 (4.194)	
Epoch: [24 / 150]   [step 40/232]	Loss1 5.957 (8.282)	Loss2 2.345 (1.440)	Loss 29.4080 (22.6840)	MAE 2.047 (2.398)	
Epoch: [24 / 150]   [step 80/232]	Loss1 26.751 (7.876)	Loss2 1.178 (1.498)	Loss 38.5348 (22.8600)	MAE 5.123 (2.296)	
Epoch: [24 / 150]   [step 120/232]	Loss1 2.252 (7.702)	Loss2 0.605 (1.425)	Loss 8.3023 (21.9560)	MAE 1.371 (2.267)	
Epoch: [24 / 150]   [step 160/232]	Loss1 1.910 (7.841)	Loss2 0.907 (1.484)	Loss 10.9746 (22.6831)	MAE 1.204 (2.277)	
Epoch: [24 / 150]   [step 200/232]	Loss1 10.845 (7.620)	Loss2 3.125 (1.501)	Loss 42.0993 (22.6316)	MAE 3.072 (2.225)	
Valid: [steps 47], Loss 889.2937,  MAE:  5.4467

*learning rate 5.00e-06*

EarlyStopping counter: 3 out of 20


Epoch: [25 / 150]   [step 0/232]	Loss1 19.651 (19.651)	Loss2 1.327 (1.327)	Loss 32.9188 (32.9188)	MAE 4.388 (4.388)	
Epoch: [25 / 150]   [step 40/232]	Loss1 24.313 (14.071)	Loss2 0.761 (1.494)	Loss 31.9219 (29.0153)	MAE 4.876 (3.104)	
Epoch: [25 / 150]   [step 80/232]	Loss1 1.482 (13.198)	Loss2 2.478 (1.260)	Loss 26.2577 (25.8020)	MAE 1.043 (2.932)	
Epoch: [25 / 150]   [step 120/232]	Loss1 0.769 (12.413)	Loss2 1.276 (1.288)	Loss 13.5265 (25.2973)	MAE 0.711 (2.853)	
Epoch: [25 / 150]   [step 160/232]	Loss1 13.043 (11.043)	Loss2 2.871 (1.253)	Loss 41.7571 (23.5707)	MAE 3.507 (2.684)	
Epoch: [25 / 150]   [step 200/232]	Loss1 4.928 (10.719)	Loss2 1.881 (1.272)	Loss 23.7417 (23.4382)	MAE 1.958 (2.651)	
Valid: [steps 47], Loss 855.8698,  MAE:  5.5682

*learning rate 5.00e-06*

EarlyStopping counter: 4 out of 20


Epoch: [26 / 150]   [step 0/232]	Loss1 0.399 (0.399)	Loss2 1.007 (1.007)	Loss 10.4666 (10.4666)	MAE 0.512 (0.512)	
Epoch: [26 / 150]   [step 40/232]	Loss1 1.069 (6.661)	Loss2 1.626 (1.027)	Loss 17.3297 (16.9306)	MAE 0.875 (2.093)	
Epoch: [26 / 150]   [step 80/232]	Loss1 2.457 (6.760)	Loss2 0.427 (0.968)	Loss 6.7242 (16.4439)	MAE 1.496 (2.122)	
Epoch: [26 / 150]   [step 120/232]	Loss1 3.397 (7.872)	Loss2 0.745 (1.000)	Loss 10.8460 (17.8733)	MAE 1.718 (2.260)	
Epoch: [26 / 150]   [step 160/232]	Loss1 12.342 (9.187)	Loss2 1.884 (1.022)	Loss 31.1792 (19.4015)	MAE 3.408 (2.424)	
Epoch: [26 / 150]   [step 200/232]	Loss1 1.513 (8.658)	Loss2 0.588 (1.013)	Loss 7.3942 (18.7829)	MAE 1.135 (2.364)	
Valid: [steps 47], Loss 796.4427,  MAE:  5.2007

*learning rate 5.00e-06*

EarlyStopping counter: 5 out of 20


Epoch: [27 / 150]   [step 0/232]	Loss1 5.728 (5.728)	Loss2 1.343 (1.343)	Loss 19.1624 (19.1624)	MAE 2.314 (2.314)	
Epoch: [27 / 150]   [step 40/232]	Loss1 8.822 (7.715)	Loss2 0.469 (0.822)	Loss 13.5135 (15.9382)	MAE 2.932 (2.252)	
Epoch: [27 / 150]   [step 80/232]	Loss1 0.415 (8.928)	Loss2 1.486 (0.772)	Loss 15.2773 (16.6508)	MAE 0.516 (2.412)	
Epoch: [27 / 150]   [step 120/232]	Loss1 0.917 (8.293)	Loss2 0.736 (0.801)	Loss 8.2791 (16.2985)	MAE 0.821 (2.303)	
Epoch: [27 / 150]   [step 160/232]	Loss1 14.015 (7.653)	Loss2 1.631 (0.790)	Loss 30.3298 (15.5509)	MAE 3.679 (2.206)	
Epoch: [27 / 150]   [step 200/232]	Loss1 0.454 (7.393)	Loss2 0.383 (0.769)	Loss 4.2822 (15.0883)	MAE 0.473 (2.173)	
Valid: [steps 47], Loss 744.4702,  MAE:  5.8479

*learning rate 2.50e-06*

EarlyStopping counter: 6 out of 20


Epoch: [28 / 150]   [step 0/232]	Loss1 1.558 (1.558)	Loss2 0.806 (0.806)	Loss 9.6152 (9.6152)	MAE 1.101 (1.101)	
Epoch: [28 / 150]   [step 40/232]	Loss1 3.552 (8.226)	Loss2 0.363 (0.648)	Loss 7.1823 (14.7063)	MAE 1.856 (2.343)	
Epoch: [28 / 150]   [step 80/232]	Loss1 0.677 (7.776)	Loss2 0.468 (0.604)	Loss 5.3621 (13.8130)	MAE 0.670 (2.252)	
Epoch: [28 / 150]   [step 120/232]	Loss1 16.802 (7.609)	Loss2 0.560 (0.600)	Loss 22.4058 (13.6056)	MAE 4.073 (2.264)	
Epoch: [28 / 150]   [step 160/232]	Loss1 5.562 (7.327)	Loss2 1.374 (0.620)	Loss 19.3039 (13.5267)	MAE 2.286 (2.223)	
Epoch: [28 / 150]   [step 200/232]	Loss1 6.376 (7.458)	Loss2 0.257 (0.619)	Loss 8.9472 (13.6516)	MAE 2.499 (2.253)	
Valid: [steps 47], Loss 800.8208,  MAE:  5.0922

*learning rate 2.50e-06*

=======>   Best at epoch 28, valid MAE 5.092214

=======>   This is the best model !!! It has been saved!!!!!!


Epoch: [29 / 150]   [step 0/232]	Loss1 0.640 (0.640)	Loss2 0.329 (0.329)	Loss 3.9333 (3.9333)	MAE 0.668 (0.668)	
Epoch: [29 / 150]   [step 40/232]	Loss1 7.495 (5.403)	Loss2 0.401 (0.498)	Loss 11.5050 (10.3824)	MAE 2.699 (1.792)	
Epoch: [29 / 150]   [step 80/232]	Loss1 0.085 (7.101)	Loss2 0.279 (0.544)	Loss 2.8776 (12.5417)	MAE 0.258 (2.118)	
Epoch: [29 / 150]   [step 120/232]	Loss1 0.699 (6.619)	Loss2 0.239 (0.523)	Loss 3.0860 (11.8522)	MAE 0.777 (2.003)	
Epoch: [29 / 150]   [step 160/232]	Loss1 16.885 (6.411)	Loss2 0.486 (0.534)	Loss 21.7424 (11.7479)	MAE 4.085 (1.995)	
Epoch: [29 / 150]   [step 200/232]	Loss1 17.231 (6.266)	Loss2 0.456 (0.536)	Loss 21.7929 (11.6244)	MAE 4.119 (1.954)	
Valid: [steps 47], Loss 723.8228,  MAE:  5.1601

*learning rate 2.50e-06*

EarlyStopping counter: 1 out of 20


Epoch: [30 / 150]   [step 0/232]	Loss1 1.153 (1.153)	Loss2 0.240 (0.240)	Loss 3.5515 (3.5515)	MAE 1.000 (1.000)	
Epoch: [30 / 150]   [step 40/232]	Loss1 0.259 (5.960)	Loss2 0.124 (0.380)	Loss 1.5024 (9.7608)	MAE 0.482 (1.892)	
Epoch: [30 / 150]   [step 80/232]	Loss1 0.139 (6.411)	Loss2 0.298 (0.434)	Loss 3.1161 (10.7466)	MAE 0.332 (2.023)	
Epoch: [30 / 150]   [step 120/232]	Loss1 5.562 (7.167)	Loss2 0.245 (0.427)	Loss 8.0145 (11.4407)	MAE 2.336 (2.131)	
Epoch: [30 / 150]   [step 160/232]	Loss1 3.275 (6.807)	Loss2 0.522 (0.446)	Loss 8.4939 (11.2674)	MAE 1.755 (2.078)	
Epoch: [30 / 150]   [step 200/232]	Loss1 2.128 (6.662)	Loss2 0.726 (0.453)	Loss 9.3829 (11.1886)	MAE 1.382 (2.060)	
Valid: [steps 47], Loss 773.7473,  MAE:  5.3762

*learning rate 2.50e-06*

EarlyStopping counter: 2 out of 20


Epoch: [31 / 150]   [step 0/232]	Loss1 0.168 (0.168)	Loss2 0.322 (0.322)	Loss 3.3871 (3.3871)	MAE 0.323 (0.323)	
Epoch: [31 / 150]   [step 40/232]	Loss1 7.014 (5.295)	Loss2 0.186 (0.380)	Loss 8.8700 (9.0915)	MAE 2.638 (1.977)	
Epoch: [31 / 150]   [step 80/232]	Loss1 21.903 (7.157)	Loss2 0.847 (0.427)	Loss 30.3719 (11.4228)	MAE 4.655 (2.231)	
Epoch: [31 / 150]   [step 120/232]	Loss1 3.014 (6.795)	Loss2 0.472 (0.420)	Loss 7.7334 (10.9929)	MAE 1.699 (2.157)	
Epoch: [31 / 150]   [step 160/232]	Loss1 0.045 (6.408)	Loss2 0.203 (0.421)	Loss 2.0743 (10.6193)	MAE 0.136 (2.078)	
Epoch: [31 / 150]   [step 200/232]	Loss1 1.059 (6.555)	Loss2 0.411 (0.421)	Loss 5.1684 (10.7702)	MAE 0.960 (2.090)	
Valid: [steps 47], Loss 816.2400,  MAE:  5.0646

*learning rate 2.50e-06*

=======>   Best at epoch 31, valid MAE 5.064581

=======>   This is the best model !!! It has been saved!!!!!!


Epoch: [32 / 150]   [step 0/232]	Loss1 23.254 (23.254)	Loss2 0.376 (0.376)	Loss 27.0175 (27.0175)	MAE 4.808 (4.808)	
Epoch: [32 / 150]   [step 40/232]	Loss1 12.815 (9.123)	Loss2 1.254 (0.418)	Loss 25.3576 (13.3033)	MAE 3.507 (2.380)	
Epoch: [32 / 150]   [step 80/232]	Loss1 3.370 (7.206)	Loss2 0.181 (0.380)	Loss 5.1801 (11.0082)	MAE 1.813 (2.109)	
Epoch: [32 / 150]   [step 120/232]	Loss1 2.439 (7.407)	Loss2 0.226 (0.398)	Loss 4.6965 (11.3914)	MAE 1.534 (2.169)	
Epoch: [32 / 150]   [step 160/232]	Loss1 12.006 (6.987)	Loss2 0.403 (0.383)	Loss 16.0385 (10.8202)	MAE 3.446 (2.154)	
Epoch: [32 / 150]   [step 200/232]	Loss1 10.487 (7.160)	Loss2 0.200 (0.404)	Loss 12.4904 (11.1990)	MAE 3.227 (2.154)	
Valid: [steps 47], Loss 810.2623,  MAE:  5.0814

*learning rate 2.50e-06*

EarlyStopping counter: 1 out of 20


Epoch: [33 / 150]   [step 0/232]	Loss1 13.302 (13.302)	Loss2 0.766 (0.766)	Loss 20.9587 (20.9587)	MAE 3.618 (3.618)	
Epoch: [33 / 150]   [step 40/232]	Loss1 4.311 (9.544)	Loss2 0.227 (0.408)	Loss 6.5836 (13.6256)	MAE 2.061 (2.426)	
Epoch: [33 / 150]   [step 80/232]	Loss1 5.473 (8.004)	Loss2 0.222 (0.366)	Loss 7.6967 (11.6636)	MAE 2.328 (2.211)	
Epoch: [33 / 150]   [step 120/232]	Loss1 9.888 (7.067)	Loss2 0.379 (0.352)	Loss 13.6760 (10.5885)	MAE 3.127 (2.085)	
Epoch: [33 / 150]   [step 160/232]	Loss1 2.102 (6.614)	Loss2 0.313 (0.339)	Loss 5.2350 (10.0011)	MAE 1.398 (2.013)	
Epoch: [33 / 150]   [step 200/232]	Loss1 3.677 (6.730)	Loss2 0.178 (0.342)	Loss 5.4585 (10.1519)	MAE 1.901 (2.038)	
Valid: [steps 47], Loss 818.5453,  MAE:  5.1636

*learning rate 2.50e-06*

EarlyStopping counter: 2 out of 20


Epoch: [34 / 150]   [step 0/232]	Loss1 15.096 (15.096)	Loss2 0.463 (0.463)	Loss 19.7255 (19.7255)	MAE 3.866 (3.866)	
Epoch: [34 / 150]   [step 40/232]	Loss1 2.980 (5.580)	Loss2 0.343 (0.323)	Loss 6.4056 (8.8068)	MAE 1.699 (2.039)	
Epoch: [34 / 150]   [step 80/232]	Loss1 0.983 (6.588)	Loss2 0.234 (0.346)	Loss 3.3260 (10.0449)	MAE 0.940 (2.239)	
Epoch: [34 / 150]   [step 120/232]	Loss1 2.020 (6.169)	Loss2 0.330 (0.347)	Loss 5.3179 (9.6432)	MAE 1.390 (2.116)	
Epoch: [34 / 150]   [step 160/232]	Loss1 0.139 (5.642)	Loss2 0.366 (0.340)	Loss 3.8032 (9.0421)	MAE 0.320 (1.968)	
Epoch: [34 / 150]   [step 200/232]	Loss1 7.250 (6.104)	Loss2 0.240 (0.339)	Loss 9.6528 (9.4930)	MAE 2.682 (2.020)	
Valid: [steps 47], Loss 885.4284,  MAE:  5.5548

*learning rate 2.50e-06*

EarlyStopping counter: 3 out of 20


Epoch: [35 / 150]   [step 0/232]	Loss1 0.430 (0.430)	Loss2 0.345 (0.345)	Loss 3.8783 (3.8783)	MAE 0.608 (0.608)	
Epoch: [35 / 150]   [step 40/232]	Loss1 2.278 (5.879)	Loss2 0.580 (0.345)	Loss 8.0794 (9.3331)	MAE 1.431 (1.958)	
Epoch: [35 / 150]   [step 80/232]	Loss1 11.892 (6.229)	Loss2 0.342 (0.338)	Loss 15.3154 (9.6117)	MAE 3.427 (1.991)	
Epoch: [35 / 150]   [step 120/232]	Loss1 3.516 (6.328)	Loss2 0.347 (0.369)	Loss 6.9820 (10.0225)	MAE 1.842 (2.046)	
Epoch: [35 / 150]   [step 160/232]	Loss1 3.985 (6.431)	Loss2 0.183 (0.361)	Loss 5.8174 (10.0372)	MAE 1.980 (1.994)	
Epoch: [35 / 150]   [step 200/232]	Loss1 4.741 (6.098)	Loss2 0.369 (0.349)	Loss 8.4356 (9.5925)	MAE 2.159 (1.956)	
Valid: [steps 47], Loss 857.9258,  MAE:  5.1191

*learning rate 2.50e-06*

EarlyStopping counter: 4 out of 20


Epoch: [36 / 150]   [step 0/232]	Loss1 11.541 (11.541)	Loss2 0.222 (0.222)	Loss 13.7604 (13.7604)	MAE 3.384 (3.384)	
Epoch: [36 / 150]   [step 40/232]	Loss1 13.003 (7.162)	Loss2 0.373 (0.287)	Loss 16.7362 (10.0339)	MAE 3.583 (2.301)	
Epoch: [36 / 150]   [step 80/232]	Loss1 0.050 (5.986)	Loss2 0.168 (0.294)	Loss 1.7287 (8.9272)	MAE 0.198 (1.948)	
Epoch: [36 / 150]   [step 120/232]	Loss1 3.116 (6.041)	Loss2 0.245 (0.286)	Loss 5.5690 (8.8971)	MAE 1.749 (1.943)	
Epoch: [36 / 150]   [step 160/232]	Loss1 7.200 (5.888)	Loss2 0.243 (0.279)	Loss 9.6267 (8.6752)	MAE 2.675 (1.913)	
Epoch: [36 / 150]   [step 200/232]	Loss1 1.291 (5.843)	Loss2 0.159 (0.285)	Loss 2.8772 (8.6905)	MAE 1.106 (1.914)	
Valid: [steps 47], Loss 808.2467,  MAE:  5.1861

*learning rate 2.50e-06*

EarlyStopping counter: 5 out of 20


Epoch: [37 / 150]   [step 0/232]	Loss1 5.219 (5.219)	Loss2 0.234 (0.234)	Loss 7.5604 (7.5604)	MAE 2.259 (2.259)	
Epoch: [37 / 150]   [step 40/232]	Loss1 26.389 (4.678)	Loss2 0.325 (0.290)	Loss 29.6366 (7.5762)	MAE 5.124 (1.734)	
Epoch: [37 / 150]   [step 80/232]	Loss1 19.616 (5.247)	Loss2 0.227 (0.295)	Loss 21.8822 (8.1930)	MAE 4.413 (1.846)	
Epoch: [37 / 150]   [step 120/232]	Loss1 13.077 (5.405)	Loss2 0.285 (0.289)	Loss 15.9314 (8.2935)	MAE 3.607 (1.856)	
Epoch: [37 / 150]   [step 160/232]	Loss1 18.568 (5.422)	Loss2 0.186 (0.295)	Loss 20.4275 (8.3769)	MAE 4.306 (1.853)	
Epoch: [37 / 150]   [step 200/232]	Loss1 5.040 (5.436)	Loss2 0.168 (0.287)	Loss 6.7201 (8.3016)	MAE 2.235 (1.847)	
Valid: [steps 47], Loss 769.2935,  MAE:  5.1553

*learning rate 1.25e-06*

EarlyStopping counter: 6 out of 20


Epoch: [38 / 150]   [step 0/232]	Loss1 4.631 (4.631)	Loss2 0.407 (0.407)	Loss 8.7039 (8.7039)	MAE 2.126 (2.126)	
Epoch: [38 / 150]   [step 40/232]	Loss1 20.756 (6.492)	Loss2 0.608 (0.279)	Loss 26.8367 (9.2870)	MAE 4.527 (2.174)	
Epoch: [38 / 150]   [step 80/232]	Loss1 1.530 (5.811)	Loss2 0.237 (0.277)	Loss 3.8973 (8.5813)	MAE 1.218 (1.993)	
Epoch: [38 / 150]   [step 120/232]	Loss1 4.393 (6.444)	Loss2 0.281 (0.274)	Loss 7.2041 (9.1815)	MAE 2.077 (2.074)	
Epoch: [38 / 150]   [step 160/232]	Loss1 2.184 (6.117)	Loss2 0.326 (0.280)	Loss 5.4451 (8.9168)	MAE 1.460 (2.009)	
Epoch: [38 / 150]   [step 200/232]	Loss1 3.896 (5.799)	Loss2 0.213 (0.282)	Loss 6.0302 (8.6159)	MAE 1.953 (1.963)	
Valid: [steps 47], Loss 816.2717,  MAE:  5.1540

*learning rate 1.25e-06*

EarlyStopping counter: 7 out of 20


Epoch: [39 / 150]   [step 0/232]	Loss1 8.560 (8.560)	Loss2 0.134 (0.134)	Loss 9.8997 (9.8997)	MAE 2.917 (2.917)	
Epoch: [39 / 150]   [step 40/232]	Loss1 5.250 (5.932)	Loss2 0.357 (0.292)	Loss 8.8169 (8.8530)	MAE 2.252 (1.993)	
Epoch: [39 / 150]   [step 80/232]	Loss1 9.580 (5.240)	Loss2 0.327 (0.275)	Loss 12.8528 (7.9899)	MAE 3.077 (1.886)	
Epoch: [39 / 150]   [step 120/232]	Loss1 0.641 (5.489)	Loss2 0.268 (0.271)	Loss 3.3160 (8.1957)	MAE 0.727 (1.879)	
Epoch: [39 / 150]   [step 160/232]	Loss1 0.176 (5.168)	Loss2 0.336 (0.266)	Loss 3.5364 (7.8303)	MAE 0.354 (1.822)	
Epoch: [39 / 150]   [step 200/232]	Loss1 4.056 (5.132)	Loss2 0.161 (0.258)	Loss 5.6657 (7.7081)	MAE 2.005 (1.819)	
Valid: [steps 47], Loss 787.6453,  MAE:  5.0750

*learning rate 1.25e-06*

EarlyStopping counter: 8 out of 20


Epoch: [40 / 150]   [step 0/232]	Loss1 0.059 (0.059)	Loss2 0.161 (0.161)	Loss 1.6729 (1.6729)	MAE 0.179 (0.179)	
Epoch: [40 / 150]   [step 40/232]	Loss1 49.744 (6.900)	Loss2 1.859 (0.294)	Loss 68.3314 (9.8445)	MAE 7.011 (2.077)	
Epoch: [40 / 150]   [step 80/232]	Loss1 0.837 (6.052)	Loss2 0.205 (0.276)	Loss 2.8823 (8.8158)	MAE 0.887 (1.931)	
Epoch: [40 / 150]   [step 120/232]	Loss1 8.649 (6.187)	Loss2 0.313 (0.273)	Loss 11.7760 (8.9171)	MAE 2.932 (1.988)	
Epoch: [40 / 150]   [step 160/232]	Loss1 3.590 (6.349)	Loss2 0.200 (0.266)	Loss 5.5892 (9.0070)	MAE 1.881 (2.022)	
Epoch: [40 / 150]   [step 200/232]	Loss1 0.986 (5.822)	Loss2 0.168 (0.264)	Loss 2.6689 (8.4635)	MAE 0.966 (1.929)	
Valid: [steps 47], Loss 676.7719,  MAE:  5.1274

*learning rate 1.25e-06*

EarlyStopping counter: 9 out of 20


Epoch: [41 / 150]   [step 0/232]	Loss1 3.496 (3.496)	Loss2 0.400 (0.400)	Loss 7.4941 (7.4941)	MAE 1.844 (1.844)	
Epoch: [41 / 150]   [step 40/232]	Loss1 7.778 (3.425)	Loss2 0.337 (0.228)	Loss 11.1456 (5.7094)	MAE 2.771 (1.535)	
Epoch: [41 / 150]   [step 80/232]	Loss1 8.001 (4.850)	Loss2 0.140 (0.268)	Loss 9.3980 (7.5271)	MAE 2.816 (1.801)	
Epoch: [41 / 150]   [step 120/232]	Loss1 5.058 (4.826)	Loss2 0.146 (0.256)	Loss 6.5177 (7.3859)	MAE 2.238 (1.778)	
Epoch: [41 / 150]   [step 160/232]	Loss1 34.241 (5.349)	Loss2 0.282 (0.255)	Loss 37.0586 (7.8990)	MAE 5.842 (1.866)	
Epoch: [41 / 150]   [step 200/232]	Loss1 0.080 (5.134)	Loss2 0.204 (0.259)	Loss 2.1195 (7.7191)	MAE 0.221 (1.812)	
Valid: [steps 47], Loss 836.7344,  MAE:  5.0866

*learning rate 1.25e-06*

EarlyStopping counter: 10 out of 20


Epoch: [42 / 150]   [step 0/232]	Loss1 0.100 (0.100)	Loss2 0.144 (0.144)	Loss 1.5369 (1.5369)	MAE 0.285 (0.285)	
Epoch: [42 / 150]   [step 40/232]	Loss1 2.279 (6.146)	Loss2 0.131 (0.227)	Loss 3.5940 (8.4115)	MAE 1.500 (1.968)	
Epoch: [42 / 150]   [step 80/232]	Loss1 3.003 (5.087)	Loss2 0.420 (0.232)	Loss 7.2004 (7.4050)	MAE 1.700 (1.766)	
Epoch: [42 / 150]   [step 120/232]	Loss1 3.001 (5.156)	Loss2 0.248 (0.231)	Loss 5.4854 (7.4686)	MAE 1.709 (1.810)	
Epoch: [42 / 150]   [step 160/232]	Loss1 0.646 (5.425)	Loss2 0.176 (0.238)	Loss 2.4029 (7.8039)	MAE 0.780 (1.869)	
Epoch: [42 / 150]   [step 200/232]	Loss1 0.253 (5.300)	Loss2 0.164 (0.235)	Loss 1.8894 (7.6522)	MAE 0.446 (1.834)	
Valid: [steps 47], Loss 815.8442,  MAE:  5.1071

*learning rate 1.25e-06*

EarlyStopping counter: 11 out of 20


Epoch: [43 / 150]   [step 0/232]	Loss1 0.321 (0.321)	Loss2 0.233 (0.233)	Loss 2.6464 (2.6464)	MAE 0.501 (0.501)	
Epoch: [43 / 150]   [step 40/232]	Loss1 2.850 (5.463)	Loss2 0.235 (0.256)	Loss 5.2044 (8.0187)	MAE 1.669 (1.907)	
Epoch: [43 / 150]   [step 80/232]	Loss1 0.625 (6.142)	Loss2 0.097 (0.246)	Loss 1.5931 (8.6019)	MAE 0.781 (2.017)	
Epoch: [43 / 150]   [step 120/232]	Loss1 6.810 (5.775)	Loss2 0.260 (0.250)	Loss 9.4070 (8.2715)	MAE 2.596 (1.964)	
Epoch: [43 / 150]   [step 160/232]	Loss1 0.129 (5.387)	Loss2 0.200 (0.250)	Loss 2.1291 (7.8877)	MAE 0.297 (1.894)	
Epoch: [43 / 150]   [step 200/232]	Loss1 9.944 (5.397)	Loss2 0.472 (0.244)	Loss 14.6614 (7.8372)	MAE 3.129 (1.904)	
Valid: [steps 47], Loss 771.1288,  MAE:  5.0763

*learning rate 6.25e-07*

EarlyStopping counter: 12 out of 20


Epoch: [44 / 150]   [step 0/232]	Loss1 0.827 (0.827)	Loss2 0.215 (0.215)	Loss 2.9765 (2.9765)	MAE 0.879 (0.879)	
Epoch: [44 / 150]   [step 40/232]	Loss1 13.646 (5.910)	Loss2 0.508 (0.226)	Loss 18.7214 (8.1669)	MAE 3.618 (1.960)	
Epoch: [44 / 150]   [step 80/232]	Loss1 0.620 (4.995)	Loss2 0.195 (0.229)	Loss 2.5657 (7.2859)	MAE 0.767 (1.785)	
Epoch: [44 / 150]   [step 120/232]	Loss1 0.147 (5.920)	Loss2 0.206 (0.249)	Loss 2.2076 (8.4129)	MAE 0.327 (1.976)	
Epoch: [44 / 150]   [step 160/232]	Loss1 0.136 (5.315)	Loss2 0.247 (0.244)	Loss 2.6099 (7.7525)	MAE 0.338 (1.833)	
Epoch: [44 / 150]   [step 200/232]	Loss1 23.362 (5.760)	Loss2 0.166 (0.249)	Loss 25.0252 (8.2465)	MAE 4.830 (1.875)	
Valid: [steps 47], Loss 786.5785,  MAE:  5.0768

*learning rate 6.25e-07*

EarlyStopping counter: 13 out of 20


Epoch: [45 / 150]   [step 0/232]	Loss1 2.071 (2.071)	Loss2 0.218 (0.218)	Loss 4.2478 (4.2478)	MAE 1.418 (1.418)	
Epoch: [45 / 150]   [step 40/232]	Loss1 6.501 (5.512)	Loss2 0.351 (0.293)	Loss 10.0135 (8.4398)	MAE 2.524 (1.956)	
Epoch: [45 / 150]   [step 80/232]	Loss1 1.533 (4.115)	Loss2 0.180 (0.251)	Loss 3.3343 (6.6232)	MAE 1.216 (1.680)	
Epoch: [45 / 150]   [step 120/232]	Loss1 4.243 (4.875)	Loss2 0.196 (0.243)	Loss 6.2006 (7.3067)	MAE 2.044 (1.752)	
Epoch: [45 / 150]   [step 160/232]	Loss1 1.167 (5.055)	Loss2 0.339 (0.244)	Loss 4.5525 (7.4915)	MAE 1.034 (1.803)	
Epoch: [45 / 150]   [step 200/232]	Loss1 0.768 (5.005)	Loss2 0.116 (0.236)	Loss 1.9290 (7.3689)	MAE 0.861 (1.790)	
Valid: [steps 47], Loss 774.8148,  MAE:  5.0890

*learning rate 6.25e-07*

EarlyStopping counter: 14 out of 20


Epoch: [46 / 150]   [step 0/232]	Loss1 5.521 (5.521)	Loss2 0.225 (0.225)	Loss 7.7733 (7.7733)	MAE 2.339 (2.339)	
Epoch: [46 / 150]   [step 40/232]	Loss1 20.141 (5.278)	Loss2 0.261 (0.225)	Loss 22.7519 (7.5318)	MAE 4.478 (1.963)	
Epoch: [46 / 150]   [step 80/232]	Loss1 4.183 (6.006)	Loss2 0.186 (0.222)	Loss 6.0452 (8.2266)	MAE 2.036 (1.975)	
Epoch: [46 / 150]   [step 120/232]	Loss1 12.782 (5.519)	Loss2 0.257 (0.225)	Loss 15.3501 (7.7696)	MAE 3.564 (1.905)	
Epoch: [46 / 150]   [step 160/232]	Loss1 2.388 (5.133)	Loss2 0.404 (0.225)	Loss 6.4237 (7.3781)	MAE 1.515 (1.801)	
Epoch: [46 / 150]   [step 200/232]	Loss1 13.387 (5.341)	Loss2 0.166 (0.226)	Loss 15.0438 (7.5980)	MAE 3.650 (1.841)	
Valid: [steps 47], Loss 725.0944,  MAE:  5.0804

*learning rate 6.25e-07*

EarlyStopping counter: 15 out of 20


Epoch: [47 / 150]   [step 0/232]	Loss1 3.796 (3.796)	Loss2 0.249 (0.249)	Loss 6.2875 (6.2875)	MAE 1.930 (1.930)	
Epoch: [47 / 150]   [step 40/232]	Loss1 1.709 (7.320)	Loss2 0.203 (0.255)	Loss 3.7376 (9.8663)	MAE 1.285 (2.147)	
Epoch: [47 / 150]   [step 80/232]	Loss1 2.590 (6.056)	Loss2 0.156 (0.240)	Loss 4.1492 (8.4555)	MAE 1.600 (1.931)	
Epoch: [47 / 150]   [step 120/232]	Loss1 4.227 (5.735)	Loss2 0.130 (0.249)	Loss 5.5265 (8.2257)	MAE 2.050 (1.881)	
Epoch: [47 / 150]   [step 160/232]	Loss1 1.921 (5.032)	Loss2 0.124 (0.231)	Loss 3.1600 (7.3456)	MAE 1.377 (1.738)	
Epoch: [47 / 150]   [step 200/232]	Loss1 13.249 (5.136)	Loss2 0.248 (0.235)	Loss 15.7331 (7.4836)	MAE 3.634 (1.739)	
Valid: [steps 47], Loss 777.4447,  MAE:  5.0720

*learning rate 6.25e-07*

EarlyStopping counter: 16 out of 20


Epoch: [48 / 150]   [step 0/232]	Loss1 2.610 (2.610)	Loss2 0.176 (0.176)	Loss 4.3680 (4.3680)	MAE 1.606 (1.606)	
Epoch: [48 / 150]   [step 40/232]	Loss1 1.801 (4.121)	Loss2 0.151 (0.245)	Loss 3.3099 (6.5699)	MAE 1.332 (1.638)	
Epoch: [48 / 150]   [step 80/232]	Loss1 6.855 (4.238)	Loss2 0.334 (0.238)	Loss 10.1973 (6.6134)	MAE 2.578 (1.635)	
Epoch: [48 / 150]   [step 120/232]	Loss1 6.446 (4.791)	Loss2 0.402 (0.230)	Loss 10.4620 (7.0919)	MAE 2.509 (1.779)	
Epoch: [48 / 150]   [step 160/232]	Loss1 1.850 (4.904)	Loss2 0.198 (0.229)	Loss 3.8282 (7.1954)	MAE 1.346 (1.770)	
Epoch: [48 / 150]   [step 200/232]	Loss1 1.419 (5.043)	Loss2 0.134 (0.227)	Loss 2.7548 (7.3180)	MAE 1.180 (1.787)	
Valid: [steps 47], Loss 867.6860,  MAE:  5.0760

*learning rate 6.25e-07*

EarlyStopping counter: 17 out of 20


Epoch: [49 / 150]   [step 0/232]	Loss1 2.505 (2.505)	Loss2 0.137 (0.137)	Loss 3.8764 (3.8764)	MAE 1.576 (1.576)	
Epoch: [49 / 150]   [step 40/232]	Loss1 4.474 (5.396)	Loss2 0.185 (0.232)	Loss 6.3196 (7.7125)	MAE 2.107 (1.857)	
Epoch: [49 / 150]   [step 80/232]	Loss1 0.843 (4.394)	Loss2 0.180 (0.216)	Loss 2.6401 (6.5551)	MAE 0.902 (1.706)	
Epoch: [49 / 150]   [step 120/232]	Loss1 0.387 (4.362)	Loss2 0.122 (0.212)	Loss 1.6073 (6.4812)	MAE 0.582 (1.670)	
Epoch: [49 / 150]   [step 160/232]	Loss1 0.069 (4.337)	Loss2 0.246 (0.215)	Loss 2.5265 (6.4856)	MAE 0.210 (1.648)	
Epoch: [49 / 150]   [step 200/232]	Loss1 0.664 (4.745)	Loss2 0.199 (0.222)	Loss 2.6571 (6.9697)	MAE 0.792 (1.680)	
Valid: [steps 47], Loss 822.2723,  MAE:  5.0870

*learning rate 3.13e-07*

EarlyStopping counter: 18 out of 20


Epoch: [50 / 150]   [step 0/232]	Loss1 0.831 (0.831)	Loss2 0.135 (0.135)	Loss 2.1849 (2.1849)	MAE 0.898 (0.898)	
Epoch: [50 / 150]   [step 40/232]	Loss1 0.784 (3.409)	Loss2 0.619 (0.219)	Loss 6.9763 (5.5967)	MAE 0.719 (1.482)	
Epoch: [50 / 150]   [step 80/232]	Loss1 0.274 (3.943)	Loss2 0.142 (0.205)	Loss 1.6927 (5.9923)	MAE 0.509 (1.567)	
Epoch: [50 / 150]   [step 120/232]	Loss1 5.662 (4.889)	Loss2 0.111 (0.234)	Loss 6.7734 (7.2272)	MAE 2.373 (1.768)	
Epoch: [50 / 150]   [step 160/232]	Loss1 15.587 (4.895)	Loss2 0.182 (0.231)	Loss 17.4069 (7.2047)	MAE 3.944 (1.764)	
Epoch: [50 / 150]   [step 200/232]	Loss1 0.022 (5.062)	Loss2 0.116 (0.229)	Loss 1.1774 (7.3503)	MAE 0.120 (1.792)	
Valid: [steps 47], Loss 736.0773,  MAE:  5.0660

*learning rate 3.13e-07*

EarlyStopping counter: 19 out of 20


Epoch: [51 / 150]   [step 0/232]	Loss1 1.975 (1.975)	Loss2 0.134 (0.134)	Loss 3.3175 (3.3175)	MAE 1.398 (1.398)	
Epoch: [51 / 150]   [step 40/232]	Loss1 0.217 (3.633)	Loss2 0.147 (0.196)	Loss 1.6851 (5.5921)	MAE 0.441 (1.590)	
Epoch: [51 / 150]   [step 80/232]	Loss1 1.248 (3.602)	Loss2 0.122 (0.195)	Loss 2.4650 (5.5541)	MAE 1.112 (1.573)	
Epoch: [51 / 150]   [step 120/232]	Loss1 2.115 (4.124)	Loss2 0.384 (0.205)	Loss 5.9562 (6.1731)	MAE 1.413 (1.631)	
Epoch: [51 / 150]   [step 160/232]	Loss1 8.564 (4.282)	Loss2 0.113 (0.211)	Loss 9.6952 (6.3881)	MAE 2.920 (1.639)	
Epoch: [51 / 150]   [step 200/232]	Loss1 2.871 (4.457)	Loss2 0.142 (0.210)	Loss 4.2929 (6.5568)	MAE 1.686 (1.689)	
Valid: [steps 47], Loss 806.3282,  MAE:  5.0732

*learning rate 3.13e-07*

EarlyStopping counter: 20 out of 20


======= Early stopping =======
Epo - Mtc
Traceback (most recent call last):
  File "/home/canhdx/workspace/TSAN-brain-age-estimation/TSAN/train_second_stage.py", line 418, in <module>
    main(res)
  File "/home/canhdx/workspace/TSAN-brain-age-estimation/TSAN/train_second_stage.py", line 172, in main
    mtc_epo = dict(zip(saved_metrics, saved_epos))
NameError: name 'saved_metrics' is not defined
